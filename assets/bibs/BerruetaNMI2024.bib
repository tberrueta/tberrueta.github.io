@Article{Berrueta2024,
author={Berrueta, Thomas A.
and Pinosky, Allison
and Murphey, Todd D.},
title={Maximum diffusion reinforcement learning},
journal={Nature Machine Intelligence},
year={2024},
month={May},
day={02},
volume={6},
number={5},
pages={504-514},
abstract={Robots and animals both experience the world through their bodies and senses. Their embodiment constrains their experiences, ensuring that they unfold continuously in space and time. As a result, the experiences of embodied agents are intrinsically correlated. Correlations create fundamental challenges for machine learning, as most techniques rely on the assumption that data are independent and identically distributed. In reinforcement learning, where data are directly collected from an agent's sequential experiences, violations of this assumption are often unavoidable. Here we derive a method that overcomes this issue by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables single-shot learning in continuous deployments over the course of individual task attempts. Moreover, we prove our approach generalizes well-known maximum entropy techniques and robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning and control form a foundation for transparent and reliable decision-making in embodied reinforcement learning agents.},
issn={2522-5839},
doi={10.1038/s42256-024-00829-3},
url={https://doi.org/10.1038/s42256-024-00829-3}
}

